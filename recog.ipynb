{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5204ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "c:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4319.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\다인\\EASYOCR-custom\\EasyOCR\\trainer\\config_files\\custom.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())\n",
      "c:\\Users\\다인\\EASYOCR-custom\\EasyOCR\\trainer\\config_files\\custom.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\EASYOCR-custom\\demo_image\n",
      "filename: 'plate.jpg', confidence: 0.9996, string: '206호1213[s][s][s][s][s][s][s][s][s][s][s][s][s][s][s][s][s][s]'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from EasyOCR.easyocr import *\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# GPU 설정\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "def get_files(path):\n",
    "    file_list = []\n",
    "\n",
    "    files = [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "    files.sort()\n",
    "    abspath = os.path.abspath(path)\n",
    "    print(abspath)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(abspath, file)\n",
    "        file_list.append(file_path)\n",
    "\n",
    "    return file_list, len(file_list)\n",
    "\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR\\\\trainer\\\\config_files',\n",
    "                # py 파일과 pth 파일, yaml 파일은 아래 이름과 똑같아야 합니다.\n",
    "                recog_network='custom')\n",
    "\n",
    "files, count = get_files('demo_image')\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    filename = os.path.basename(file)\n",
    "    # 경로에 한글이 들어간다면 에러 발생 가능성이 높습니다.\n",
    "    img = cv2.imread(r'./demo_image/plate.jpg')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    result = reader.recognize(gray)\n",
    "\n",
    "    # ./easyocr/utils.py 733 lines\n",
    "    # result[0]: bbox\n",
    "    # result[1]: string\n",
    "    # result[2]: confidence\n",
    "    for (bbox, string, confidence) in result:\n",
    "        print(\"filename: '%s', confidence: %.4f, string: '%s'\" % (filename, confidence, string))\n",
    "        # print('bbox: ', bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d914bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'recognize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ✅ recognizer.recognize() 사용 - 검출 없이 인식만\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m string, confidence \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m(gray)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, confidence: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m, string: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename, confidence, string))\n",
      "File \u001b[1;32mc:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1964\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1963\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1964\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1966\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'recognize'"
     ]
    }
   ],
   "source": [
    "from easyocr import Reader\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "def get_files(path):\n",
    "    file_list = []\n",
    "    files = [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "    files.sort()\n",
    "    abspath = os.path.abspath(path)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(abspath, file)\n",
    "        file_list.append(file_path)\n",
    "    return file_list, len(file_list)\n",
    "\n",
    "# Reader 초기화\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR\\\\trainer\\\\config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "files, count = get_files('demo_image')\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    filename = os.path.basename(file)\n",
    "    img = cv2.imread(r'./demo_image/plate.jpg')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # ✅ recognizer.recognize() 사용 - 검출 없이 인식만\n",
    "    string, confidence = reader.recognizer.recognize(gray)\n",
    "    \n",
    "    print(\"filename: '%s', confidence: %.4f, string: '%s'\" % (filename, confidence, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641c5919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'recognize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ✅ recognizer의 recognize 메서드 사용\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m(\n\u001b[0;32m     34\u001b[0m     gray,\n\u001b[0;32m     35\u001b[0m     decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     36\u001b[0m     beamWidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     37\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     38\u001b[0m     contrast_ths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     39\u001b[0m     adjust_contrast\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     40\u001b[0m     filter_ths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# result 구조 확인\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1964\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1963\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1964\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1966\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'recognize'"
     ]
    }
   ],
   "source": [
    "from easyocr import Reader\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "def get_files(path):\n",
    "    file_list = []\n",
    "    files = [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "    files.sort()\n",
    "    abspath = os.path.abspath(path)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(abspath, file)\n",
    "        file_list.append(file_path)\n",
    "    return file_list, len(file_list)\n",
    "\n",
    "# Reader 초기화\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR\\\\trainer\\\\config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "recognizer = reader.recognizer\n",
    "\n",
    "files, count = get_files('demo_image')\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    filename = os.path.basename(file)\n",
    "    img = cv2.imread(r'./demo_image/plate.jpg')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # ✅ recognizer의 recognize 메서드 사용\n",
    "    result = recognizer.recognize(\n",
    "        gray,\n",
    "        decoder='greedy',\n",
    "        beamWidth=5,\n",
    "        batch_size=1,\n",
    "        contrast_ths=0.1,\n",
    "        adjust_contrast=0.5,\n",
    "        filter_ths=0.003\n",
    "    )\n",
    "    \n",
    "    # result 구조 확인\n",
    "    print(f\"Result type: {type(result)}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    \n",
    "    # EasyOCR의 recognize는 리스트를 반환할 수 있음\n",
    "    if isinstance(result, list) and len(result) > 0:\n",
    "        string = result[0][0] if isinstance(result[0], tuple) else result[0]\n",
    "        confidence = result[0][1] if isinstance(result[0], tuple) and len(result[0]) > 1 else 0.0\n",
    "    else:\n",
    "        string = result\n",
    "        confidence = 0.0\n",
    "    \n",
    "    print(\"filename: '%s', confidence: %.4f, string: '%s'\" % (filename, confidence, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e7f541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Reader 속성 ===\n",
      "character: <class 'str'>\n",
      "converter: <class 'easyocr.utils.CTCLabelConverter'>\n",
      "cudnn_benchmark: <class 'bool'>\n",
      "detect: <class 'method'>\n",
      "detect_network: <class 'str'>\n",
      "detection_models: <class 'dict'>\n",
      "detector: <class 'easyocr.craft.CRAFT'>\n",
      "device: <class 'str'>\n",
      "download_enabled: <class 'bool'>\n",
      "getChar: <class 'method'>\n",
      "getDetectorPath: <class 'method'>\n",
      "get_detector: <class 'function'>\n",
      "get_textbox: <class 'function'>\n",
      "initDetector: <class 'method'>\n",
      "lang_char: <class 'str'>\n",
      "model_lang: <class 'str'>\n",
      "model_storage_directory: <class 'str'>\n",
      "quantize: <class 'tuple'>\n",
      "readtext: <class 'method'>\n",
      "readtext_batched: <class 'method'>\n",
      "readtextlang: <class 'method'>\n",
      "recognition_models: <class 'dict'>\n",
      "recognize: <class 'method'>\n",
      "recognizer: <class 'custom.Model'>\n",
      "setDetector: <class 'method'>\n",
      "setLanguageList: <class 'method'>\n",
      "setModelLanguage: <class 'method'>\n",
      "support_detection_network: <class 'list'>\n",
      "user_network_directory: <class 'str'>\n",
      "verbose: <class 'bool'>\n",
      "\n",
      "=== recognize 메서드 찾기 ===\n",
      "✅ reader.recognize 존재!\n",
      "Type: <class 'method'>\n"
     ]
    }
   ],
   "source": [
    "from easyocr import Reader\n",
    "\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR\\\\trainer\\\\config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "# Reader의 모든 속성 확인\n",
    "print(\"=== Reader 속성 ===\")\n",
    "for attr in dir(reader):\n",
    "    if not attr.startswith('_'):\n",
    "        obj = getattr(reader, attr)\n",
    "        print(f\"{attr}: {type(obj)}\")\n",
    "\n",
    "print(\"\\n=== recognize 메서드 찾기 ===\")\n",
    "# Reader에 직접 recognize가 있는지\n",
    "if hasattr(reader, 'recognize'):\n",
    "    print(\"✅ reader.recognize 존재!\")\n",
    "    print(f\"Type: {type(reader.recognize)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3a0c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 같은 이미지 5번 실행 ===\n",
      "\n",
      "1회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "2회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "3회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "4회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "5회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "\n",
      "→ 5번 모두 동일해야 정상입니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from easyocr import Reader\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# 재현성 보장\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "\n",
    "# Reader 초기화\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR/trainer/config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "# eval 모드 강제 설정\n",
    "reader.recognizer.eval()\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"CustomOCR과 동일한 전처리\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    img_pil = Image.fromarray(gray).resize((100, 32), Image.BICUBIC)\n",
    "    return np.array(img_pil)\n",
    "\n",
    "# 이미지 로드\n",
    "img = cv2.imread('./demo_image/plate.jpg')\n",
    "processed = preprocess(img)\n",
    "\n",
    "# 여러 번 실행해서 결과 확인\n",
    "print(\"=== 같은 이미지 5번 실행 ===\\n\")\n",
    "for i in range(5):\n",
    "    result = reader.recognize(\n",
    "        processed,\n",
    "        horizontal_list=None,\n",
    "        free_list=None,\n",
    "        detail=1,\n",
    "        adjust_contrast=0\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        text = result[0][1]\n",
    "        conf = result[0][2]\n",
    "        print(f\"{i+1}회: '{text}' (신뢰도: {conf:.6f})\")\n",
    "    else:\n",
    "        print(f\"{i+1}회: 인식 실패\")\n",
    "\n",
    "print(\"\\n→ 5번 모두 동일해야 정상입니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f5993e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 레이어:\n",
      "module.Transformation.LocalizationNetwork.conv.0.weight\n",
      "module.Transformation.LocalizationNetwork.conv.1.weight\n",
      "module.Transformation.LocalizationNetwork.conv.1.bias\n",
      "module.Transformation.LocalizationNetwork.conv.1.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.1.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.1.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.conv.4.weight\n",
      "module.Transformation.LocalizationNetwork.conv.5.weight\n",
      "module.Transformation.LocalizationNetwork.conv.5.bias\n",
      "module.Transformation.LocalizationNetwork.conv.5.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.5.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.5.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.conv.8.weight\n",
      "module.Transformation.LocalizationNetwork.conv.9.weight\n",
      "module.Transformation.LocalizationNetwork.conv.9.bias\n",
      "module.Transformation.LocalizationNetwork.conv.9.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.9.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.9.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.conv.12.weight\n",
      "module.Transformation.LocalizationNetwork.conv.13.weight\n",
      "module.Transformation.LocalizationNetwork.conv.13.bias\n",
      "module.Transformation.LocalizationNetwork.conv.13.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.13.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.13.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.localization_fc1.0.weight\n",
      "module.Transformation.LocalizationNetwork.localization_fc1.0.bias\n",
      "module.Transformation.LocalizationNetwork.localization_fc2.weight\n",
      "module.Transformation.LocalizationNetwork.localization_fc2.bias\n",
      "module.Transformation.GridGenerator.inv_delta_C\n",
      "module.Transformation.GridGenerator.P_hat\n",
      "module.FeatureExtraction.ConvNet.conv0_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_1.bias\n",
      "module.FeatureExtraction.ConvNet.bn0_1.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn0_1.running_var\n",
      "module.FeatureExtraction.ConvNet.bn0_1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv0_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_2.bias\n",
      "module.FeatureExtraction.ConvNet.bn0_2.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn0_2.running_var\n",
      "module.FeatureExtraction.ConvNet.bn0_2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer1.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer1.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.0.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.bias\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.0.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.1.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.1.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.0.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.1.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.1.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.2.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.2.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.3.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.3.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.4.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.4.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv3.weight\n",
      "module.FeatureExtraction.ConvNet.bn3.weight\n",
      "module.FeatureExtraction.ConvNet.bn3.bias\n",
      "module.FeatureExtraction.ConvNet.bn3.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn3.running_var\n",
      "module.FeatureExtraction.ConvNet.bn3.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.1.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.1.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.2.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.2.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv4_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_1.bias\n",
      "module.FeatureExtraction.ConvNet.bn4_1.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn4_1.running_var\n",
      "module.FeatureExtraction.ConvNet.bn4_1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv4_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_2.bias\n",
      "module.FeatureExtraction.ConvNet.bn4_2.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn4_2.running_var\n",
      "module.FeatureExtraction.ConvNet.bn4_2.num_batches_tracked\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse\n",
      "module.SequenceModeling.0.linear.weight\n",
      "module.SequenceModeling.0.linear.bias\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse\n",
      "module.SequenceModeling.1.linear.weight\n",
      "module.SequenceModeling.1.linear.bias\n",
      "module.Prediction.attention_cell.i2h.weight\n",
      "module.Prediction.attention_cell.h2h.weight\n",
      "module.Prediction.attention_cell.h2h.bias\n",
      "module.Prediction.attention_cell.score.weight\n",
      "module.Prediction.attention_cell.rnn.weight_ih\n",
      "module.Prediction.attention_cell.rnn.weight_hh\n",
      "module.Prediction.attention_cell.rnn.bias_ih\n",
      "module.Prediction.attention_cell.rnn.bias_hh\n",
      "module.Prediction.generator.weight\n",
      "module.Prediction.generator.bias\n",
      "\n",
      "TPS 있음: True\n",
      "ResNet 있음: False\n",
      "VGG 있음: False\n"
     ]
    }
   ],
   "source": [
    "# best.pth가 어떤 구조로 학습되었는지 확인\n",
    "import torch\n",
    "\n",
    "checkpoint = torch.load('./model/custom.pth', map_location='cpu')\n",
    "\n",
    "# 모델 키 확인\n",
    "print(\"모델 레이어:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(key)\n",
    "    \n",
    "# Transformation 레이어가 있는지 확인\n",
    "has_tps = any('Transformation' in key for key in checkpoint.keys())\n",
    "has_resnet = any('FeatureExtraction.ConvNet.resnet' in key for key in checkpoint.keys())\n",
    "has_vgg = any('FeatureExtraction.ConvNet.VGG' in key for key in checkpoint.keys())\n",
    "\n",
    "print(f\"\\nTPS 있음: {has_tps}\")\n",
    "print(f\"ResNet 있음: {has_resnet}\")\n",
    "print(f\"VGG 있음: {has_vgg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64191631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 구조 분석 ===\n",
      "\n",
      "Transformation: TPS\n",
      "\n",
      "FeatureExtraction 레이어:\n",
      "  module.FeatureExtraction.ConvNet.conv0_1.weight\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.weight\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.bias\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.running_mean\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.running_var\n",
      "\n",
      "  → FeatureExtraction: Unknown (첫 레이어 확인 필요)\n",
      "\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "\n",
      "=== 전체 모듈 구조 ===\n",
      "  - FeatureExtraction\n",
      "  - Prediction\n",
      "  - SequenceModeling\n",
      "  - Transformation\n",
      "\n",
      "=== 추천 config.yaml 설정 ===\n",
      "\n",
      "ocr:\n",
      "  path: \"models/ocr/best.pth\"\n",
      "  config:\n",
      "    img_height: 32\n",
      "    img_width: 100\n",
      "    input_channel: 1\n",
      "    output_channel: 256\n",
      "    hidden_size: 256\n",
      "    num_fiducial: 20\n",
      "    transformation: \"TPS\"\n",
      "    feature_extraction: \"Unknown (첫 레이어 확인 필요)\"  # ← 확인 필요!\n",
      "    sequence_modeling: \"BiLSTM\"\n",
      "    prediction: \"CTC\"\n",
      "    batch_max_length: 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('./model/custom.pth', map_location='cpu')\n",
    "\n",
    "print(\"=== 모델 구조 분석 ===\\n\")\n",
    "\n",
    "# 1. Transformation 확인\n",
    "has_tps = any('Transformation' in key for key in checkpoint.keys())\n",
    "print(f\"Transformation: {'TPS' if has_tps else 'None'}\")\n",
    "\n",
    "# 2. FeatureExtraction 확인\n",
    "feature_keys = [key for key in checkpoint.keys() if 'FeatureExtraction' in key]\n",
    "if feature_keys:\n",
    "    print(\"\\nFeatureExtraction 레이어:\")\n",
    "    for key in feature_keys[:5]:  # 처음 5개만\n",
    "        print(f\"  {key}\")\n",
    "    \n",
    "    # 어떤 타입인지 추측\n",
    "    if any('resnet' in key.lower() for key in feature_keys):\n",
    "        feature_type = \"ResNet\"\n",
    "    elif any('vgg' in key.lower() for key in feature_keys):\n",
    "        feature_type = \"VGG\"\n",
    "    elif any('rcnn' in key.lower() for key in feature_keys):\n",
    "        feature_type = \"RCNN\"\n",
    "    else:\n",
    "        feature_type = \"Unknown (첫 레이어 확인 필요)\"\n",
    "    \n",
    "    print(f\"\\n  → FeatureExtraction: {feature_type}\")\n",
    "\n",
    "# 3. SequenceModeling 확인\n",
    "has_lstm = any('SequenceModeling' in key and 'LSTM' in key for key in checkpoint.keys())\n",
    "has_bilstm = any('SequenceModeling' in key for key in checkpoint.keys())\n",
    "print(f\"\\nSequenceModeling: {'BiLSTM' if has_bilstm else ('LSTM' if has_lstm else 'None')}\")\n",
    "\n",
    "# 4. Prediction 확인\n",
    "has_attention = any('Attention' in key for key in checkpoint.keys())\n",
    "has_ctc = any('CTCLoss' in key or 'generator' in key for key in checkpoint.keys())\n",
    "\n",
    "if has_attention:\n",
    "    prediction_type = \"Attn\"\n",
    "elif has_ctc:\n",
    "    prediction_type = \"CTC\"\n",
    "else:\n",
    "    prediction_type = \"Unknown\"\n",
    "\n",
    "print(f\"Prediction: {prediction_type}\")\n",
    "\n",
    "# 5. 전체 모듈 확인\n",
    "print(\"\\n=== 전체 모듈 구조 ===\")\n",
    "modules = set()\n",
    "for key in checkpoint.keys():\n",
    "    parts = key.split('.')\n",
    "    if len(parts) > 1:\n",
    "        modules.add(parts[1])  # module. 다음 부분\n",
    "\n",
    "for module in sorted(modules):\n",
    "    print(f\"  - {module}\")\n",
    "\n",
    "print(\"\\n=== 추천 config.yaml 설정 ===\")\n",
    "print(f\"\"\"\n",
    "ocr:\n",
    "  path: \"models/ocr/best.pth\"\n",
    "  config:\n",
    "    img_height: 32\n",
    "    img_width: 100\n",
    "    input_channel: 1\n",
    "    output_channel: 256\n",
    "    hidden_size: 256\n",
    "    num_fiducial: 20\n",
    "    transformation: \"TPS\"\n",
    "    feature_extraction: \"{feature_type}\"  # ← 확인 필요!\n",
    "    sequence_modeling: \"BiLSTM\"\n",
    "    prediction: \"{prediction_type}\"\n",
    "    batch_max_length: 25\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
