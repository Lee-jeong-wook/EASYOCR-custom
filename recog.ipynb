{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5204ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\EASYOCR-custom\\demo_image\n",
      "filename: 'objectid_21659344_1761203704061.jpg', confidence: 0.0000, string: '름귀실귀파주파주파'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from easyocr.easyocr import *\n",
    "import cv2\n",
    "\n",
    "# GPU 설정\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "def get_files(path):\n",
    "    file_list = []\n",
    "\n",
    "    files = [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "    files.sort()\n",
    "    abspath = os.path.abspath(path)\n",
    "    print(abspath)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(abspath, file)\n",
    "        file_list.append(file_path)\n",
    "\n",
    "    return file_list, len(file_list)\n",
    "\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR\\\\trainer\\\\config_files',\n",
    "                # py 파일과 pth 파일, yaml 파일은 아래 이름과 똑같아야 합니다.\n",
    "                recog_network='custom')\n",
    "\n",
    "files, count = get_files('demo_image')\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    filename = os.path.basename(file)\n",
    "    # 경로에 한글이 들어간다면 에러 발생 가능성이 높습니다.\n",
    "    img = cv2.imread(r'./demo_image/objectid_21659344_1761203704061.jpg')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    result = reader.readtext(gray)\n",
    "\n",
    "    # ./easyocr/utils.py 733 lines\n",
    "    # result[0]: bbox\n",
    "    # result[1]: string\n",
    "    # result[2]: confidence\n",
    "    for (bbox, string, confidence) in result:\n",
    "        print(\"filename: '%s', confidence: %.4f, string: '%s'\" % (filename, confidence, string))\n",
    "        # print('bbox: ', bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d914bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 크기: 640x320\n",
      "처리 후: 100x32\n",
      "\n",
      "텍스트: '206모1213' | 신뢰도: 0.3223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from easyocr import Reader\n",
    "\n",
    "# GPU 설정\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"이미지 전처리\"\"\"\n",
    "    # 크기 확대 (2배)\n",
    "    h, w = img.shape[:2]\n",
    "    img = cv2.resize(img, (100, 32), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    return gray\n",
    "\n",
    "# Reader 초기화\n",
    "# reader = Reader(['ko'], gpu=True,\n",
    "#                 model_storage_directory='model',\n",
    "#                 user_network_directory='EasyOCR/trainer/config_files',\n",
    "#                 recog_network='custom')\n",
    "reader = Reader(['ko'])\n",
    "\n",
    "# 이미지 로드\n",
    "img_path = './demo_image/plate.jpg'\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# 전처리\n",
    "gray = preprocess_image(img)\n",
    "\n",
    "# OCR 실행\n",
    "result = reader.readtext(\n",
    "    gray,\n",
    "    detail=1,\n",
    "    contrast_ths=0.1,\n",
    "    adjust_contrast=0.5\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"원본 크기: {img.shape[1]}x{img.shape[0]}\")\n",
    "print(f\"처리 후: {processed.shape[1]}x{processed.shape[0]}\\n\")\n",
    "\n",
    "if result:\n",
    "    for bbox, text, conf in result:\n",
    "        print(f\"텍스트: '{text}' | 신뢰도: {conf:.4f}\")\n",
    "else:\n",
    "    print(\"인식된 텍스트 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "641c5919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인식 결과: 구중제영8다8은8다 (신뢰도: 0.0000)\n",
      "plate.jpg: '구중제영8다8은8다' (신뢰도: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from easyocr import Reader\n",
    "from PIL import Image\n",
    "\n",
    "# Reader 초기화\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR/trainer/config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"모델 학습 크기(100x32)로 전처리\"\"\"\n",
    "    # Grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # 가벼운 대비 향상\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # 100x32로 고정 resize\n",
    "    img_pil = Image.fromarray(enhanced).resize((100, 32), Image.BICUBIC)\n",
    "    \n",
    "    return np.array(img_pil)\n",
    "\n",
    "# 단일 이미지 처리\n",
    "img = cv2.imread('./demo_image/plate.jpg')\n",
    "processed = preprocess_image(img)\n",
    "\n",
    "result = reader.recognize(\n",
    "    processed,\n",
    "    horizontal_list=None,  # 전체 영역\n",
    "    free_list=None,\n",
    "    detail=1,\n",
    "    adjust_contrast=0,     # 추가 대비 조정 끄기\n",
    "    contrast_ths=0\n",
    ")\n",
    "\n",
    "if result:\n",
    "    text = result[0][1]\n",
    "    conf = result[0][2]\n",
    "    print(f\"인식 결과: {text} (신뢰도: {conf:.4f})\")\n",
    "else:\n",
    "    print(\"인식 실패\")\n",
    "\n",
    "# ========================================\n",
    "# 여러 이미지 배치 처리\n",
    "# ========================================\n",
    "import os\n",
    "\n",
    "image_dir = './demo_image'\n",
    "files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "for filename in files:\n",
    "    img_path = os.path.join(image_dir, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    processed = preprocess_image(img)\n",
    "    result = reader.recognize(processed, horizontal_list=None, \n",
    "                             free_list=None, adjust_contrast=0)\n",
    "    \n",
    "    if result:\n",
    "        text = result[0][1]\n",
    "        conf = result[0][2]\n",
    "        print(f\"{filename}: '{text}' (신뢰도: {conf:.4f})\")\n",
    "    else:\n",
    "        print(f\"{filename}: 인식 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e7f541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: 640x320\n",
      "처리: 100x32\n",
      "\n",
      "텍스트: '누하모음명음모9' | 신뢰도: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from easyocr import Reader\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# Reader 초기화\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR/trainer/config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"CustomOCR과 동일한 전처리\"\"\"\n",
    "    # Grayscale\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # PIL로 100x32 resize\n",
    "    img_pil = Image.fromarray(gray).resize((100, 32), Image.BICUBIC)\n",
    "    \n",
    "    return np.array(img_pil)\n",
    "\n",
    "# 이미지 로드\n",
    "img = cv2.imread('./demo_image/plate.jpg')\n",
    "\n",
    "# 전처리\n",
    "processed = preprocess(img)\n",
    "\n",
    "# OCR 실행\n",
    "result = reader.recognize(\n",
    "    processed,\n",
    "    horizontal_list=None,\n",
    "    free_list=None,\n",
    "    detail=1,\n",
    "    adjust_contrast=0\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"원본: {img.shape[1]}x{img.shape[0]}\")\n",
    "print(f\"처리: {processed.shape[1]}x{processed.shape[0]}\\n\")\n",
    "\n",
    "if result:\n",
    "    text = result[0][1]\n",
    "    conf = result[0][2]\n",
    "    print(f\"텍스트: '{text}' | 신뢰도: {conf:.4f}\")\n",
    "else:\n",
    "    print(\"인식 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3a0c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 같은 이미지 5번 실행 ===\n",
      "\n",
      "1회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "2회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "3회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "4회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "5회: '오외초외등계안시안' (신뢰도: 0.000000)\n",
      "\n",
      "→ 5번 모두 동일해야 정상입니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\다인\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from easyocr import Reader\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# 재현성 보장\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "\n",
    "# Reader 초기화\n",
    "reader = Reader(['ko'], gpu=True,\n",
    "                model_storage_directory='model',\n",
    "                user_network_directory='EasyOCR/trainer/config_files',\n",
    "                recog_network='custom')\n",
    "\n",
    "# eval 모드 강제 설정\n",
    "reader.recognizer.eval()\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"CustomOCR과 동일한 전처리\"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    img_pil = Image.fromarray(gray).resize((100, 32), Image.BICUBIC)\n",
    "    return np.array(img_pil)\n",
    "\n",
    "# 이미지 로드\n",
    "img = cv2.imread('./demo_image/plate.jpg')\n",
    "processed = preprocess(img)\n",
    "\n",
    "# 여러 번 실행해서 결과 확인\n",
    "print(\"=== 같은 이미지 5번 실행 ===\\n\")\n",
    "for i in range(5):\n",
    "    result = reader.recognize(\n",
    "        processed,\n",
    "        horizontal_list=None,\n",
    "        free_list=None,\n",
    "        detail=1,\n",
    "        adjust_contrast=0\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        text = result[0][1]\n",
    "        conf = result[0][2]\n",
    "        print(f\"{i+1}회: '{text}' (신뢰도: {conf:.6f})\")\n",
    "    else:\n",
    "        print(f\"{i+1}회: 인식 실패\")\n",
    "\n",
    "print(\"\\n→ 5번 모두 동일해야 정상입니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f5993e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 레이어:\n",
      "module.Transformation.LocalizationNetwork.conv.0.weight\n",
      "module.Transformation.LocalizationNetwork.conv.1.weight\n",
      "module.Transformation.LocalizationNetwork.conv.1.bias\n",
      "module.Transformation.LocalizationNetwork.conv.1.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.1.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.1.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.conv.4.weight\n",
      "module.Transformation.LocalizationNetwork.conv.5.weight\n",
      "module.Transformation.LocalizationNetwork.conv.5.bias\n",
      "module.Transformation.LocalizationNetwork.conv.5.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.5.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.5.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.conv.8.weight\n",
      "module.Transformation.LocalizationNetwork.conv.9.weight\n",
      "module.Transformation.LocalizationNetwork.conv.9.bias\n",
      "module.Transformation.LocalizationNetwork.conv.9.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.9.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.9.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.conv.12.weight\n",
      "module.Transformation.LocalizationNetwork.conv.13.weight\n",
      "module.Transformation.LocalizationNetwork.conv.13.bias\n",
      "module.Transformation.LocalizationNetwork.conv.13.running_mean\n",
      "module.Transformation.LocalizationNetwork.conv.13.running_var\n",
      "module.Transformation.LocalizationNetwork.conv.13.num_batches_tracked\n",
      "module.Transformation.LocalizationNetwork.localization_fc1.0.weight\n",
      "module.Transformation.LocalizationNetwork.localization_fc1.0.bias\n",
      "module.Transformation.LocalizationNetwork.localization_fc2.weight\n",
      "module.Transformation.LocalizationNetwork.localization_fc2.bias\n",
      "module.Transformation.GridGenerator.inv_delta_C\n",
      "module.Transformation.GridGenerator.P_hat\n",
      "module.FeatureExtraction.ConvNet.conv0_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_1.bias\n",
      "module.FeatureExtraction.ConvNet.bn0_1.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn0_1.running_var\n",
      "module.FeatureExtraction.ConvNet.bn0_1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv0_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn0_2.bias\n",
      "module.FeatureExtraction.ConvNet.bn0_2.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn0_2.running_var\n",
      "module.FeatureExtraction.ConvNet.bn0_2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer1.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer1.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer1.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.0.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.weight\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.bias\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer1.0.downsample.1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.0.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.0.downsample.1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.1.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer2.1.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer2.1.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.0.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.0.downsample.1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.1.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.1.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.1.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.2.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.2.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.2.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.3.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.3.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.3.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.4.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer3.4.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer3.4.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv3.weight\n",
      "module.FeatureExtraction.ConvNet.bn3.weight\n",
      "module.FeatureExtraction.ConvNet.bn3.bias\n",
      "module.FeatureExtraction.ConvNet.bn3.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn3.running_var\n",
      "module.FeatureExtraction.ConvNet.bn3.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.0.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.0.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.0.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.1.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.1.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.1.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.2.conv1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.layer4.2.conv2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.weight\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.bias\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.running_mean\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.running_var\n",
      "module.FeatureExtraction.ConvNet.layer4.2.bn2.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv4_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_1.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_1.bias\n",
      "module.FeatureExtraction.ConvNet.bn4_1.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn4_1.running_var\n",
      "module.FeatureExtraction.ConvNet.bn4_1.num_batches_tracked\n",
      "module.FeatureExtraction.ConvNet.conv4_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_2.weight\n",
      "module.FeatureExtraction.ConvNet.bn4_2.bias\n",
      "module.FeatureExtraction.ConvNet.bn4_2.running_mean\n",
      "module.FeatureExtraction.ConvNet.bn4_2.running_var\n",
      "module.FeatureExtraction.ConvNet.bn4_2.num_batches_tracked\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0\n",
      "module.SequenceModeling.0.rnn.weight_ih_l0_reverse\n",
      "module.SequenceModeling.0.rnn.weight_hh_l0_reverse\n",
      "module.SequenceModeling.0.rnn.bias_ih_l0_reverse\n",
      "module.SequenceModeling.0.rnn.bias_hh_l0_reverse\n",
      "module.SequenceModeling.0.linear.weight\n",
      "module.SequenceModeling.0.linear.bias\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0\n",
      "module.SequenceModeling.1.rnn.weight_ih_l0_reverse\n",
      "module.SequenceModeling.1.rnn.weight_hh_l0_reverse\n",
      "module.SequenceModeling.1.rnn.bias_ih_l0_reverse\n",
      "module.SequenceModeling.1.rnn.bias_hh_l0_reverse\n",
      "module.SequenceModeling.1.linear.weight\n",
      "module.SequenceModeling.1.linear.bias\n",
      "module.Prediction.attention_cell.i2h.weight\n",
      "module.Prediction.attention_cell.h2h.weight\n",
      "module.Prediction.attention_cell.h2h.bias\n",
      "module.Prediction.attention_cell.score.weight\n",
      "module.Prediction.attention_cell.rnn.weight_ih\n",
      "module.Prediction.attention_cell.rnn.weight_hh\n",
      "module.Prediction.attention_cell.rnn.bias_ih\n",
      "module.Prediction.attention_cell.rnn.bias_hh\n",
      "module.Prediction.generator.weight\n",
      "module.Prediction.generator.bias\n",
      "\n",
      "TPS 있음: True\n",
      "ResNet 있음: False\n",
      "VGG 있음: False\n"
     ]
    }
   ],
   "source": [
    "# best.pth가 어떤 구조로 학습되었는지 확인\n",
    "import torch\n",
    "\n",
    "checkpoint = torch.load('./model/custom.pth', map_location='cpu')\n",
    "\n",
    "# 모델 키 확인\n",
    "print(\"모델 레이어:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(key)\n",
    "    \n",
    "# Transformation 레이어가 있는지 확인\n",
    "has_tps = any('Transformation' in key for key in checkpoint.keys())\n",
    "has_resnet = any('FeatureExtraction.ConvNet.resnet' in key for key in checkpoint.keys())\n",
    "has_vgg = any('FeatureExtraction.ConvNet.VGG' in key for key in checkpoint.keys())\n",
    "\n",
    "print(f\"\\nTPS 있음: {has_tps}\")\n",
    "print(f\"ResNet 있음: {has_resnet}\")\n",
    "print(f\"VGG 있음: {has_vgg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64191631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 구조 분석 ===\n",
      "\n",
      "Transformation: TPS\n",
      "\n",
      "FeatureExtraction 레이어:\n",
      "  module.FeatureExtraction.ConvNet.conv0_1.weight\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.weight\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.bias\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.running_mean\n",
      "  module.FeatureExtraction.ConvNet.bn0_1.running_var\n",
      "\n",
      "  → FeatureExtraction: Unknown (첫 레이어 확인 필요)\n",
      "\n",
      "SequenceModeling: BiLSTM\n",
      "Prediction: CTC\n",
      "\n",
      "=== 전체 모듈 구조 ===\n",
      "  - FeatureExtraction\n",
      "  - Prediction\n",
      "  - SequenceModeling\n",
      "  - Transformation\n",
      "\n",
      "=== 추천 config.yaml 설정 ===\n",
      "\n",
      "ocr:\n",
      "  path: \"models/ocr/best.pth\"\n",
      "  config:\n",
      "    img_height: 32\n",
      "    img_width: 100\n",
      "    input_channel: 1\n",
      "    output_channel: 256\n",
      "    hidden_size: 256\n",
      "    num_fiducial: 20\n",
      "    transformation: \"TPS\"\n",
      "    feature_extraction: \"Unknown (첫 레이어 확인 필요)\"  # ← 확인 필요!\n",
      "    sequence_modeling: \"BiLSTM\"\n",
      "    prediction: \"CTC\"\n",
      "    batch_max_length: 25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load('./model/custom.pth', map_location='cpu')\n",
    "\n",
    "print(\"=== 모델 구조 분석 ===\\n\")\n",
    "\n",
    "# 1. Transformation 확인\n",
    "has_tps = any('Transformation' in key for key in checkpoint.keys())\n",
    "print(f\"Transformation: {'TPS' if has_tps else 'None'}\")\n",
    "\n",
    "# 2. FeatureExtraction 확인\n",
    "feature_keys = [key for key in checkpoint.keys() if 'FeatureExtraction' in key]\n",
    "if feature_keys:\n",
    "    print(\"\\nFeatureExtraction 레이어:\")\n",
    "    for key in feature_keys[:5]:  # 처음 5개만\n",
    "        print(f\"  {key}\")\n",
    "    \n",
    "    # 어떤 타입인지 추측\n",
    "    if any('resnet' in key.lower() for key in feature_keys):\n",
    "        feature_type = \"ResNet\"\n",
    "    elif any('vgg' in key.lower() for key in feature_keys):\n",
    "        feature_type = \"VGG\"\n",
    "    elif any('rcnn' in key.lower() for key in feature_keys):\n",
    "        feature_type = \"RCNN\"\n",
    "    else:\n",
    "        feature_type = \"Unknown (첫 레이어 확인 필요)\"\n",
    "    \n",
    "    print(f\"\\n  → FeatureExtraction: {feature_type}\")\n",
    "\n",
    "# 3. SequenceModeling 확인\n",
    "has_lstm = any('SequenceModeling' in key and 'LSTM' in key for key in checkpoint.keys())\n",
    "has_bilstm = any('SequenceModeling' in key for key in checkpoint.keys())\n",
    "print(f\"\\nSequenceModeling: {'BiLSTM' if has_bilstm else ('LSTM' if has_lstm else 'None')}\")\n",
    "\n",
    "# 4. Prediction 확인\n",
    "has_attention = any('Attention' in key for key in checkpoint.keys())\n",
    "has_ctc = any('CTCLoss' in key or 'generator' in key for key in checkpoint.keys())\n",
    "\n",
    "if has_attention:\n",
    "    prediction_type = \"Attn\"\n",
    "elif has_ctc:\n",
    "    prediction_type = \"CTC\"\n",
    "else:\n",
    "    prediction_type = \"Unknown\"\n",
    "\n",
    "print(f\"Prediction: {prediction_type}\")\n",
    "\n",
    "# 5. 전체 모듈 확인\n",
    "print(\"\\n=== 전체 모듈 구조 ===\")\n",
    "modules = set()\n",
    "for key in checkpoint.keys():\n",
    "    parts = key.split('.')\n",
    "    if len(parts) > 1:\n",
    "        modules.add(parts[1])  # module. 다음 부분\n",
    "\n",
    "for module in sorted(modules):\n",
    "    print(f\"  - {module}\")\n",
    "\n",
    "print(\"\\n=== 추천 config.yaml 설정 ===\")\n",
    "print(f\"\"\"\n",
    "ocr:\n",
    "  path: \"models/ocr/best.pth\"\n",
    "  config:\n",
    "    img_height: 32\n",
    "    img_width: 100\n",
    "    input_channel: 1\n",
    "    output_channel: 256\n",
    "    hidden_size: 256\n",
    "    num_fiducial: 20\n",
    "    transformation: \"TPS\"\n",
    "    feature_extraction: \"{feature_type}\"  # ← 확인 필요!\n",
    "    sequence_modeling: \"BiLSTM\"\n",
    "    prediction: \"{prediction_type}\"\n",
    "    batch_max_length: 25\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
